---
layout:     post
title:      "神经网络初步理解"
subtitle: 	"使用数学方法形式上表示深度学习模型"
date:       2021-06-23
author:     "hstk30"
tags:
    - 数学分析
    - 深度学习
---

> 道可道，非常道。名可名，非常名。无名，万物之始也；有名，万物之母也。故恒无欲也，以观其眇；恒有欲也，以观其所徼。两者同出，异名同谓。玄之又玄，众眇之门。  
  

好像还没人能解释深度学习模型（Explainable ML）[^ML-Lee]，只知道这样是可以工作的、有效的，并且再这么处理一下应该能工作的更好。在接触深度学习一个月后，终于明白为什么干这行的人会自嘲为**炼丹师**。 

下面是我对神经网络现阶段（一个月）的理解: 

## 连续函数的近似逼近

其实深度学习做的事就是希望通过给定大量数据，让程序自己找到一个好一点的函数，使得在未知的数据上，这个函数也能表现的不会太差。  
抽象成数学语言就是：给出一堆离散的点 \\( (\vec{x_i}, y_i) \\) ，找到一条连续的曲线 \\( f(\vec{x}) \\) 使得尽量多的点在这条曲线上或附近，即 
\\[ loss = \sum(f(\vec{x_i}) -y_i) ^ 2 \\] 
尽量小，也可以使用其他`loss` 进行衡量。

### 三角函数逼近 Fourier级数

> 在任意区间上任意给定的函数属于极广泛一类者（例如分段可微分或分段单调的函数等），可以展开为三角级数。

这是大学高等数学必学的知识：函数的傅里叶级数展开。用公式来表示即：
\\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^{\infty} ( a_n \cos nx + b_n \sin nx ) \\]
其中
\\[ a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\cos nx dx \\]  \\[ b_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\sin nx dx  \\]

\\( f(x) \\) 的傅里叶级数在满足**收敛定理**（迪尼-利普希茨判别法或狄利克雷-若尔当判别法）时取等号。对于给定的有限个点，我们可以默认拟合的函数是符合**收敛定理**，即\\( f(\vec{x}) \\)收敛于它的傅里叶级数。


### 多项式逼近 Weierstrass定理[^math]

> 我们可将在区间[a, b] 上的连续函数f(x) 展开为一致收敛的级数，其中各项是整代数多项式。

即

> 定理： 如果函数 f(x) 在区间[a, b] 上连续，则无论数 \\( \varepsilon > 0 \\) 是怎样，可找到这样的整代数多项式
>  \\[ f(x) = c_0 + c_i x + c_2 x^2 + ... + c_n x^n \\] 
> 使得关于在[a, b] 上x 的一切值，不等式 \\[ |f(x) - P(x)| < \varepsilon \\] 一致成立。

这定理其实很好证明，或者说是很好理解。因为上面我们说 f(x) 可以展开为三角多项式
\\[ T(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} ( a_n \cos nx + b_n \sin nx ) \\]
如果将T 中的每个三角函数用它的x 的幂级数展开式来代替，即：
\\[ \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} + ... \\]
\\[ \cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} + ... \\]
则也可将函数T 表示为处处收敛的幂级数：
\\[ T(x) = \sum_{m=0}^{\infty} c_m x^m \\]
这个级数在对应区间上也一致收敛，因此当n 充分大时， 有
\\[ |f(x) - P(x)| < |f(x) - T(x)| + |T(x) - P(x)| < \varepsilon \\]

### 计算多项式的霍纳规则 Horner's rule[^algo]

上面说了这么多数学相关的，其实就是想说：  
**我们希望程序帮我们找到的这个好的连续函数可以表示为多项式的形式。**  
或者说  
**我们就想找个多项式函数**。  

霍纳规则是说：
> \\( P(x) = \sum_{k=0}^{n} a_k a^k  = a_0 + x (a_1 + x (a_2+ ... + x (a_{n-1} + x a_n)))  \\)

这样计算多项式的值，只需要n 次加法，和n 次乘法，比直接算多项式少了多次乘法的开销。  

如果将上面的x 换成深度学习里的权重矩阵\\( w_i \\), \\( a_i \\) 换成bia 向量，那么
\\[ P(w) = b_0 + w_0 (b_1 + w_1 (b_2 + ... + w_2 (b_{n-1} + w_n b_n)))\\]
再把 \\( b_n \\) 换成我们的输入特征向量x，则  
\\[  P(w) = b_0 + w_0 (b_1 + w_1 (b_2 + ... + w_2 (b_{n-1} + w_n x))) \\]
这样看的话是不是和深度学习里的全连接前馈网络（fully connected feedforward network）很像，假设f(x) 恒大于0，那么激活函数Relu （Rectified Linear Unit）的作用就相当于恒等变换。

### 结尾

以上只是形式上的一种近似（瞎扯的），只是最近我在学习深度学习时，结合以前学过的知识联想到的，强行说服自己：深度学习还是可解释的🙏🙏🙏


### 参考

[Machine Learning (2021) Mandarin Version](https://www.youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J)  
[为什么都说神经网络是个黑箱？](https://www.zhihu.com/question/263672028)  
 
[^ML-Lee]: [李宏毅机器学习](https://www.youtube.com/watch?v=WQY85vaQfTI&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=27&ab_channel=Hung-yiLee)  
[^math]: 《微积分学教程》 菲赫金哥尔茨 第三卷  第十九章 傅里叶级数
[^algo]: 《算法导论》第二版 第二章
