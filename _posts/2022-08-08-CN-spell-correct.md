---
layout:     post
title:      "中文拼写纠错"
date:       2022-08-08
author:     "hstk30"
header-img: "img/arrive.jpg"
tags:
    - 深度学习
--- 

# 中文拼写纠错

## 数据的收集

- 字形，需要有个常用字的字形字典
- 字音，需要有个常用字的字音字典，包括多音字
- 常用错词的语义，根据训练好的词向量计算相似度得到语义相似的错词
- 键盘键位
- 常用知识，实体识别一些专有名词


## 数据构造

2023.3.29更新：

在大概两年的深度学习的工作经历中，其实我的大部分工作就是处理数据：

- 想办法构造数据
- 想办法清洗已有数据
- 想办法挖掘数据


而这个构造数据算是比较考创造力的，最简单的构造数据的方法就是找人工标，
但是成本太高，没有冤大头老板会支持这种方法。

所以数据构造的一个我认为最重要的方法就是：
设计规则打乱原有的大语料，从而得到想要的符合特定要求的数据对。

这里面的要点

1. 设计规则，这里其实才是深度学习里最体现创造力的地方（对我这种不是专业做研究的人来说）。
2. 大数据，大数据保证这里面的数据大部分都是正常的，存在的噪音会被大数据所覆盖、忽略。


Leader 说的： **一个模型的效果60%来自数据，30%来自规则，10%来自模型的结构。**


## 训练

- 一份大语料，一个基本的假设是这个大语料中的消息基本上没有错别字
- 一份易错字、词替换字典，可以根据字形、字音、字意构造


### 构造数据

- input: 随机替换一条消息中的某些字为对应的易错字
- target: 原消息
- loss: 对于被替换的位置上的分数计算一个loss

直接上`bert` 作为一个大分类(词表大小)任务 `硬train 一发👊`，


## 一些注意事项

- 训练时的采样问题，因为易错字的分布也是极其不均衡的，因此采样的时候应该平均一点
- 裁剪`bert` 词表，`bert` 词表大小2w，中文常用字3-4千，通用字7千，可以裁掉`bert` 里的各种日文韩文和英语词汇
- 分词直接按字力度分词即可
- 训练量比较大，`batch_size` 设不了很高，使用梯度累加更新效果应该会好一下

## 参考

[FASPell](https://aclanthology.org/D19-5522/)

